{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8037d622",
   "metadata": {},
   "source": [
    "for more scope creep, using a 30 (or 60 or 90) day price plot estimate the most likley day that the stock will peak and use this as the sell date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc70cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7918520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import pandas_datareader as pdr\n",
    "from datetime import datetime, timedelta, time\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "#from alpha_vantage.techindicators import TechIndicators\n",
    "from pandas.tseries.offsets import BMonthEnd\n",
    "#from dotenv import load_dotenv\n",
    "import os\n",
    "import pickle\n",
    "import talib\n",
    "#import tensorflow as tf\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix # Loading required libraries\n",
    "\n",
    "#load_dotenv()\n",
    "#ALPHA_V_API_KEY = os.getenv('ALPHA_V_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72053cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2015_2025_data/insider_trades.csv', parse_dates=['transaction_date', 'trade_date'])\n",
    "df = df.dropna()\n",
    "\n",
    "for i in ['last_price', 'Qty', 'shares_held', 'Owned', 'Value']:\n",
    "    df[i] = (\n",
    "        df[i]\n",
    "        .astype(str)\n",
    "        .str.replace('New', '1000000000', regex=False)\n",
    "        .str.replace('[\\$,\\+\\-\\%>]', '', regex=True)\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "df['transaction_date'] = pd.to_datetime(df['transaction_date'], format=\"%d/%m/%Y %H:%M\")\n",
    "df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "\n",
    "#print(type(df['transaction_date']))\n",
    "#print(df['transaction_date'])\n",
    "#print(df['trade_date'])\n",
    "\n",
    "df['Owned_norm'] = 1 - np.exp(-df['Owned']*np.log(2)/100)\n",
    "\n",
    "df = df.sort_values(by='transaction_date')\n",
    "\n",
    "#df = df.loc[:499]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b3611",
   "metadata": {},
   "source": [
    "### Saving and loading varilabes with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d88baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs('saved_vars', exist_ok=True)  # create folder if it doesn't exist\n",
    "\n",
    "'''\n",
    "Saving a variable\n",
    "'''\n",
    "#with open('saved_vars/prices_for_all_tickers.pkl', 'wb') as f:\n",
    "#    pickle.dump(prices, f)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Loading a variable\n",
    "'''\n",
    "#with open('saved_vars/500_trades_from_mid_2024.pkl', 'rb') as f:  # 'rb' = read binary\n",
    "#    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed685b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_vars/500_trades_from_mid_2024.pkl', 'rb') as f:  # 'rb' = read binary\n",
    "    data = pickle.load(f)\n",
    "df_by_ticker = data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c408ba1",
   "metadata": {},
   "source": [
    "#### you can look at the recommendations by analysts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f487cd3",
   "metadata": {},
   "source": [
    "### Grouping tickers\n",
    "\n",
    "Ticker\tCompany Name\tUnique Buyers\tTotal Purchases\tPositions Represented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970912fb",
   "metadata": {},
   "source": [
    "#### Improved grouping.\n",
    "\n",
    "1: Instead of looping from index i to index j where j has date i plus two days, a mask is used with a date and ticker condition.\n",
    "\n",
    "2: Instead of using a pd series for used indexs, a set() was added to by taking the .index list from the masked pd df (group_df).\n",
    "\n",
    "3: The unique_buyers and all_titles were created by taking sets of the cols of group_df.\n",
    "\n",
    "4: A dict was then created with the *final* numbers and then added to the list of grouped trades, and then that list was converted to a pd df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac594af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = df.sort_values(by='transaction_date').reset_index(drop=True)\n",
    "grouped = []\n",
    "used_indices = set()\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if idx in used_indices:\n",
    "        continue\n",
    "\n",
    "    ticker = row['ticker']\n",
    "    start_date = row['transaction_date']\n",
    "    cutoff_date = start_date + timedelta(days=5)\n",
    "\n",
    "    # Filter trades for this ticker within the 2-day window\n",
    "    mask = (\n",
    "        (df['ticker'] == ticker) &\n",
    "        (df['transaction_date'] >= start_date) &\n",
    "        (df['transaction_date'] <= cutoff_date)\n",
    "    )\n",
    "    group_df = df[mask]\n",
    "    group_indices = group_df.index.tolist() \n",
    "    used_indices.update(group_indices)\n",
    "\n",
    "    unique_buyers = set(group_df['owner_name'])\n",
    "    all_titles = set(group_df['Title'])\n",
    "\n",
    "    earliest_trade_date = group_df['trade_date'].min()\n",
    "\n",
    "    unique_buyers_count = len(unique_buyers) \n",
    "\n",
    "    \n",
    "\n",
    "    grouped.append({\n",
    "        'transaction_date': start_date,\n",
    "        'trade_date': earliest_trade_date,\n",
    "        'ticker': ticker,\n",
    "        'company_name': row['company_name'],\n",
    "        'unique_buyers': unique_buyers_count,\n",
    "        'total_purchases': len(group_df),\n",
    "        'Title': list(all_titles),\n",
    "        'owner_names': list(unique_buyers)\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "df_by_ticker = pd.DataFrame(grouped) #note that trade_date is the earliest transaction \n",
    "#print(df_by_ticker)\n",
    "\n",
    "def adjust_transaction_date(dt):\n",
    "    if dt.hour < 9 or (dt.hour == 9 and dt.minute < 30):\n",
    "        return (dt - pd.Timedelta(days=1)).date()\n",
    "    else:\n",
    "        return dt.date()\n",
    "\n",
    "df_by_ticker['eff_trans_date'] = df_by_ticker['transaction_date'].apply(adjust_transaction_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15113ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50699654",
   "metadata": {},
   "source": [
    "### Retreiving ticker prices and key metrics\n",
    "\n",
    "Binary up down for the next day \n",
    "\n",
    "We want to find the percent change over the following day, 5 days, 20 days (excluding weekends).\n",
    "\n",
    "Keep a 30 day price plot maybe for predicting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e82a8",
   "metadata": {},
   "source": [
    "for more scope creep, using a 30 (or 60 or 90) day price plot estimate the most likley day that the stock will peak and use this as the sell date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ceed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was slow and trash, better to bulk download and select the 90 days after\n",
    "'''\n",
    "trade = df_by_ticker.iloc[1]\n",
    "print(trade)\n",
    "start= trade['transaction_date'].date()  - timedelta(days=91)\n",
    "end = trade['transaction_date'].date() - timedelta(days=1)\n",
    "\n",
    "ticker = yf.Ticker(trade['ticker'])\n",
    "ticker_data = ticker.history(start=start, end=end)[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "def get_ticker_data_90_days(trade): #pass in row of df that incrudes the trade date and the ticker symbol\n",
    "    #print(trade)\n",
    "    start= trade['transaction_date'].date()  - timedelta(days=91)\n",
    "    end = trade['transaction_date'].date() - timedelta(days=1)\n",
    "    return yf.Ticker(trade['ticker']).history(start=start, end=end)[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "\n",
    "df_ticker_data = df_by_ticker.apply(lambda row: get_ticker_data_90_days(row), axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1835a04",
   "metadata": {},
   "source": [
    "#### Bulk download ticker data and then pickout the 90 day bits you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb14615",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = df_by_ticker['ticker'].unique()\n",
    "\n",
    "# Bulk download full date range covering all needed periods\n",
    "# any missing failed downloads with still have a row but poplated with NaN's\n",
    "start=df_by_ticker['eff_trans_date'].min() - timedelta(days=100)\n",
    "end=df_by_ticker['eff_trans_date'].max() + timedelta(days=31)\n",
    "\n",
    "downloaded_prices = yf.download(\n",
    "    tickers=list(tickers),\n",
    "    start=start,\n",
    "    end=end\n",
    ")[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "spy_history = yf.Ticker('SPY').history(start=start, end=end)[['Open', 'High', 'Low', 'Close', 'Volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfda84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Slice per row from this cached dataset\n",
    "def get_ticker_data_from_cache(trade):\n",
    "    prev_price = downloaded_prices.loc[trade['eff_trans_date'] - timedelta(days=100):trade['eff_trans_date'], (slice(None), trade['ticker'])]\n",
    "    prev_price.columns = prev_price.columns.droplevel(1)\n",
    "\n",
    "    future_price = downloaded_prices.loc[trade['eff_trans_date']+ timedelta(days=1) : trade['eff_trans_date'] + timedelta(days=31), (slice(None), trade['ticker'])]\n",
    "    future_price.columns = future_price.columns.droplevel(1)\n",
    "\n",
    "    row = {\n",
    "        'ticker': trade['ticker'],\n",
    "        'prev_prices': prev_price,\n",
    "        'future_prices' : future_price\n",
    "    }\n",
    "    return row\n",
    "\n",
    "df_ticker_data = df_by_ticker.apply(get_ticker_data_from_cache, axis=1, result_type='expand')\n",
    "\n",
    "df_by_ticker['prev_prices'] = df_ticker_data['prev_prices']\n",
    "df_by_ticker['future_prices'] = df_ticker_data['future_prices']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298f498",
   "metadata": {},
   "source": [
    "### Another ticker download method\n",
    "#### Since the above method will download 10 years worth of ticker data for 150K trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_ranges = df.groupby(\"ticker\")[\"trade_date\"].agg(\n",
    "    min_date=lambda x: x.min() - pd.DateOffset(months=4),\n",
    "    max_date=lambda x: x.max() + pd.DateOffset(months=2)\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a701ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = {}\n",
    "\n",
    "for _, row in ticker_ranges.iterrows():\n",
    "    t = row[\"ticker\"]\n",
    "    start, end = row[\"min_date\"], row[\"max_date\"]\n",
    "    try:\n",
    "        data = yf.download(t, start=start, end=end)[['Open','High','Low','Close','Volume']]\n",
    "        prices[t] = data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed {t}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a8390",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = ticker_ranges.iloc[351]\n",
    "start, end = row[\"min_date\"], row[\"max_date\"]\n",
    "t = row['ticker']\n",
    "print('##########################', t, start, end)\n",
    "ticker_data = yf.Ticker(t).history(start=start, end=end)[['Open', 'High', 'Low', 'Close', 'Volume']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0995221",
   "metadata": {},
   "source": [
    "### Download tickers in batches of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_ranges = ticker_ranges.sort_values(\"min_date\").reset_index(drop=True)\n",
    "asdfg\n",
    "batch_size = 200\n",
    "prices = {}\n",
    "\n",
    "for i in range(0, len(ticker_ranges), batch_size):\n",
    "    batch = ticker_ranges.iloc[i:i+batch_size]\n",
    "    tickers = batch['ticker'].tolist()\n",
    "    batch_start = batch['min_date'].min()\n",
    "    batch_end   = batch['max_date'].max()\n",
    "    \n",
    "    try:\n",
    "        data = yf.download(\n",
    "            tickers,\n",
    "            start=batch_start,\n",
    "            end=batch_end\n",
    "        )[['Open','High','Low','Close','Volume']]\n",
    "        \n",
    "        # store each tickerâ€™s slice separately\n",
    "        for t in tickers:\n",
    "            if (t,) in data.columns:  # multiindex form\n",
    "                prices[t] = data[t]\n",
    "            else:  # single ticker form\n",
    "                prices[t] = data\n",
    "    except Exception as e:\n",
    "        print(f\"Batch failed {tickers[:5]}...: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913d9ed",
   "metadata": {},
   "source": [
    "#### Load in prices from saved_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd014cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_vars/prices_for_all_tickers.pkl', 'rb') as f:  # 'rb' = read binary\n",
    "    prices = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e85629",
   "metadata": {},
   "source": [
    "## Creating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_features(trade):\n",
    "    \"\"\"\n",
    "    ticker_data: DataFrame with OHLCV for one stock over 3 months\n",
    "    Returns: Single row of engineered features:\n",
    "        '1mo_return', '3mo_return', '30d_volatility', 'rsi_14', 'macd',\n",
    "        'volume_zscore', 'price_vs_sma50'.\n",
    "    \"\"\"\n",
    "    ticker_data = trade['prev_prices']\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    features['trade_date_epoch'] = pd.to_datetime(trade['trade_date']).timestamp() # seconds since epoch\n",
    "    features['eff_trans_date_epoch'] = pd.to_datetime(trade['eff_trans_date']).timestamp() \n",
    "\n",
    "    \n",
    "    # Price returns\n",
    "    features['1mo_return'] = ticker_data['Close'].iloc[-1] / ticker_data['Close'].iloc[-22] - 1\n",
    "    features['3mo_return'] = ticker_data['Close'].iloc[-1] / ticker_data['Close'].iloc[-63] - 1\n",
    "    \n",
    "    # Volatility\n",
    "    features['30d_volatility'] = ticker_data['Close'].pct_change().std() * np.sqrt(252)\n",
    "    \n",
    "    # Momentum indicators\n",
    "    features['rsi_14'] = talib.RSI(ticker_data['Close'], timeperiod=14)[-1]\n",
    "    features['macd'] = talib.MACD(ticker_data['Close'])[0][-1]  # MACD line\n",
    "    \n",
    "    # Volume\n",
    "    features['volume_zscore'] = (\n",
    "        (ticker_data['Volume'].iloc[-1] - ticker_data['Volume'].mean()) \n",
    "        / ticker_data['Volume'].std()\n",
    "    )\n",
    "    \n",
    "    # Trend relationships\n",
    "    features['price_vs_sma50'] = ticker_data['Close'].iloc[-1] / ticker_data['Close'].rolling(50).mean().iloc[-1]\n",
    "\n",
    "    # Bool on whether the transaction was made within trading hours.\n",
    "    features['is_during_market_hours'] = time(hour=9, minute=30) < trade['transaction_date'].time() < time(hour=16)\n",
    "\n",
    "    # Day of the week the trade was made on.\n",
    "    features['day_of_week'] = trade['trade_date'].dayofweek\n",
    "\n",
    "    # Number of days between making the trade and filing with the SEC\n",
    "    features['filing_lad_days'] = (trade['transaction_date']-trade['trade_date']).days\n",
    "\n",
    "    #df_by_ticker = df_by_ticker.dropna()\n",
    "    \n",
    "    title_str = ''.join(trade['Title'])\n",
    "    if 'CEO' in title_str:\n",
    "        features['title_rank'] = 4\n",
    "    elif 'C' in title_str:\n",
    "        features['title_rank'] = 3\n",
    "    elif 'Dir' in title_str:\n",
    "        features['title_rank'] = 2\n",
    "    else:\n",
    "        features['title_rank'] = 1\n",
    "        \n",
    "    return pd.Series(features)\n",
    "\n",
    "\n",
    "def flag_month_end(trade_dates):\n",
    "    \"\"\"Returns 1 for last 3 business days of month\"\"\"\n",
    "    flags = []\n",
    "    for date in trade_dates:\n",
    "        month_end = BMonthEnd().rollforward(date)  # Get official month-end\n",
    "        last_3_days = pd.bdate_range(end=month_end, periods=3)  # Last 3 trading days\n",
    "        flags.append(1 if date in last_3_days else 0)\n",
    "    return flags\n",
    "\n",
    "features = df_by_ticker.apply(lambda trade: create_features(trade), axis=1)\n",
    "\n",
    "\n",
    "# Looks at the last valid trading date on or before the transaction date.\n",
    "def get_spy_return(effective_date, spy_history):\n",
    "    \"\"\"Returns SPY's 1-day return for the last valid trading day <= effective_date\"\"\"\n",
    "    # Get all SPY dates <= effective_date\n",
    "    valid_dates = spy_history.index[spy_history.index.date <= effective_date]\n",
    "    \n",
    "    if len(valid_dates) == 0:\n",
    "        return np.nan  # No previous trading data\n",
    "    \n",
    "    # Take the closest trading date\n",
    "    dt = valid_dates[-1]\n",
    "    return (spy_history.loc[dt, 'Close'] / spy_history.loc[dt, 'Open']) - 1\n",
    "\n",
    "df_by_ticker['SPY_1d_return'] = df_by_ticker['eff_trans_date'].apply(\n",
    "    lambda x: get_spy_return(x, spy_history)\n",
    ")\n",
    "\n",
    "\n",
    "df_by_ticker[features.columns] = features\n",
    "df_by_ticker['month_end_flag'] = flag_month_end(df_by_ticker['trade_date'])\n",
    "df_by_ticker = df_by_ticker.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04bb41c",
   "metadata": {},
   "source": [
    "### Create target variable for price movement more than x%\n",
    "#### True for increase, False for decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b40ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_targets(prices):\n",
    "    targets = {}\n",
    "\n",
    "    targets['next_day'] = 1 < prices['Close'].iloc[0]/prices['Open'].iloc[0]\n",
    "    targets['one_week'] = 1 < prices['Close'].iloc[5]/prices['Open'].iloc[0]\n",
    "    targets['one_month'] = 1 < prices['Close'].iloc[20]/prices['Open'].iloc[0] #weekends not included => 1 week = 5 elements\n",
    "\n",
    "    targets['next_day_2%'] = 1.02 < prices['Close'].iloc[0]/prices['Open'].iloc[0]\n",
    "    targets['one_week_2%'] = 1.02 < prices['Close'].iloc[5]/prices['Open'].iloc[0]\n",
    "    targets['one_month_2%'] = 1.02 < prices['Close'].iloc[20]/prices['Open'].iloc[0]\n",
    "\n",
    "\n",
    "    targets['next_day_5%'] = 1.05 < prices['Close'].iloc[0]/prices['Open'].iloc[0]\n",
    "    targets['one_week_5%'] = 1.05 < prices['Close'].iloc[5]/prices['Open'].iloc[0]\n",
    "    targets['one_month_5%'] = 1.05 < prices['Close'].iloc[20]/prices['Open'].iloc[0]\n",
    "\n",
    "\n",
    "    return pd.Series(targets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "targets = df_by_ticker.apply(lambda trade: create_targets(trade['future_prices']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25abad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_ticker.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf13b4f",
   "metadata": {},
   "source": [
    "## prediction 1 day higher (1) lower (0) price movements "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e39828",
   "metadata": {},
   "source": [
    "\n",
    "### (think about making that higher or lower than a x% increase, whatever the cut off is)\n",
    "\n",
    "1. Binomial Logistic Regression (Good Baseline): An issue with this is that not all variables will be linearly independant. Good to get as many finacial ratios and indicators leading up to the purchase. But still do real basic analysis.\n",
    "\n",
    "2. Random Forest: better for nonlin relationships, machine learning supervised classification modele.\n",
    "\n",
    "****** 3. XGBoost is best \"XGBoost with well-engineered features outperforms both binomial regression and deep learning in production.\", best with tabular data. Min samples 5k, ideal >50k sample.\n",
    "\n",
    "4. LSTM, for time series data (price data leading up to the date of purchase.) clean data, GPU resources, >50,000 samples required\n",
    "\n",
    "\n",
    "\n",
    "#### To be added\n",
    "days_since_earnings\n",
    "\n",
    "Sector_ETF_5d_trend\n",
    "\n",
    "VIX_level\n",
    "\n",
    "This could be good to put in as well only if the filing_lag_days has some significance:\n",
    "\n",
    "```\n",
    "df['urgent_filing'] = (df['filing_lag_days'] <= 1).astype(int)  # Binary flag\n",
    "df['lag_x_volume'] = df['filing_lag_days'] * df['trade_volume']  # Interaction term\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61914968",
   "metadata": {},
   "source": [
    "#### GPT bullhsit you could add but i can't be bothered rn\n",
    "\n",
    "### **1. Insider-Specific Features**\n",
    "1. **`insider_cluster_size`**  \n",
    "   - Number of unique insiders trading the same stock within 5 days  \n",
    "   ```python\n",
    "   df['insider_cluster_size'] = df.groupby(['ticker', pd.Grouper(key='trade_date', freq='5D')])['owner_names'].transform('nunique')\n",
    "   ```\n",
    "\n",
    "2. **`title_rank`**  \n",
    "   - Numeric hierarchy of insider titles (CEO=4, CFO=3, Director=2, Other=1)  \n",
    "   ```python\n",
    "   title_rank = {'CEO':4, 'CFO':3, 'Director':2}\n",
    "   df['title_rank'] = df['Title'].map(title_rank).fillna(1)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Advanced Market Context**\n",
    "3. **`sector_relative_strength`**  \n",
    "   - Stock's 5-day return vs sector ETF (e.g., XLK for tech)  \n",
    "   ```python\n",
    "   df['sector_relative_strength'] = df['1mo_return'] - sector_etf_returns\n",
    "   ```\n",
    "\n",
    "4. **`vix_1d_change`**  \n",
    "   - Daily % change in VIX (volatility spike indicator)  \n",
    "   ```python\n",
    "   df['vix_1d_change'] = vix_data['Close'].pct_change().loc[df['eff_trans_date']].values\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Technical Enhancements**\n",
    "5. **`atr_14`**  \n",
    "   - Average True Range (volatility normalization)  \n",
    "   ```python\n",
    "   df['atr_14'] = talib.ATR(df['High'], df['Low'], df['Close'], 14)\n",
    "   ```\n",
    "\n",
    "6. **`obv_5d`**  \n",
    "   - 5-day On-Balance Volume trend  \n",
    "   ```python\n",
    "   df['obv_5d'] = talib.OBV(df['Close'], df['Volume']).pct_change(5)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Behavioral Signals**\n",
    "7. **`short_interest_ratio`**  \n",
    "   - Short interest / float (squeeze potential)  \n",
    "   ```python\n",
    "   df['short_interest_ratio'] = df['ticker'].map(short_interest_data)\n",
    "   ```\n",
    "\n",
    "8. **`earnings_proximity`**  \n",
    "   - Days until next earnings (-30 to +30, 0=earnings day)  \n",
    "   ```python\n",
    "   df['earnings_proximity'] = (earnings_dates - df['trade_date']).dt.days\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Risk Management**\n",
    "9. **`beta_1mo`**  \n",
    "   - Stock's 1-month beta to SPY  \n",
    "   ```python\n",
    "   df['beta_1mo'] = df['ticker'].map(beta_calculations)\n",
    "   ```\n",
    "\n",
    "10. **`liquidity_zscore`**  \n",
    "    - Current volume vs 3-month avg (z-score)  \n",
    "    ```python\n",
    "    df['liquidity_zscore'] = (df['Volume'] - df['Volume'].rolling(63).mean()) / df['Volume'].rolling(63).std()\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3632bf08",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = targets['next_day_2%']\n",
    "X = df_by_ticker.drop(['transaction_date', 'trade_date', 'eff_trans_date', 'ticker','company_name', 'owner_names', 'prev_prices', 'future_prices', 'Title'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Spliting Train Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_leaf_nodes=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=90,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac867099",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "confusion_mtx = confusion_matrix(y_test, y_pred) \n",
    "plot_confusion_matrix(confusion_mtx, classes = range(2)) \n",
    "# Confusion Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aecad86",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0653b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botenv (3.9.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
